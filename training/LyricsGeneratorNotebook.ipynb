{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('Data/BloodyCromLyrics.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the goblin creeper\\nforged by elves\\nis the sword of thorin\\nis the tusk of rage\\n\\nthe sword of king und'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 27, 28, 10, 18, 12, 23, 17, 29, 24, 10,  2, 21, 28, 28, 30, 28,\n",
       "       21, 31,  3, 12, 21, 18, 28,  6, 10, 23,  4, 10, 28, 17, 11, 28,  5,\n",
       "       31, 29,  5, 10, 25, 27, 28, 10,  5, 13, 12, 21,  6, 10, 12,  3, 10,\n",
       "       25, 27, 12, 21, 29, 24, 31, 29,  5, 10, 25, 27, 28, 10, 25, 16,  5,\n",
       "        0, 10, 12,  3, 10, 21,  7, 18, 28, 31, 31, 25, 27, 28, 10,  5, 13,\n",
       "       12, 21,  6, 10, 12,  3, 10,  0, 29, 24, 18, 10, 16, 24,  6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[25 27 28 10 18 12 23 17 29 24]\n",
      " [10 30  7  5 25 31 31 24 12 13]\n",
      " [12 21 18 12 25 27 10 27  7 14]\n",
      " [27 28 10 23  7 25 25 17 28 10]\n",
      " [ 7 11 28 10  2 16 25 10 25 27]\n",
      " [12  5 31  0 29 17 17 10 25 27]\n",
      " [21 12 25 31 21 28  7 17 14  5]\n",
      " [10 27  7 11 28 10 25 12 10 21]]\n",
      "\n",
      "y\n",
      " [[27 28 10 18 12 23 17 29 24 10]\n",
      " [30  7  5 25 31 31 24 12 13 10]\n",
      " [21 18 12 25 27 10 27  7 14 14]\n",
      " [28 10 23  7 25 25 17 28 10 29]\n",
      " [11 28 10  2 16 25 10 25 27 28]\n",
      " [ 5 31  0 29 17 17 10 25 27 28]\n",
      " [12 25 31 21 28  7 17 14  5 10]\n",
      " [27  7 11 28 10 25 12 10 21 28]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.05, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(32, 150, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=150, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden = 150\n",
    "n_layers = 3\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50... Step: 10... Loss: 3.1994... Val Loss: 3.1658\n",
      "Epoch: 1/50... Step: 20... Loss: 3.0101... Val Loss: 3.0605\n",
      "Epoch: 2/50... Step: 30... Loss: 3.0880... Val Loss: 3.0270\n",
      "Epoch: 2/50... Step: 40... Loss: 3.0622... Val Loss: 3.0086\n",
      "Epoch: 3/50... Step: 50... Loss: 2.9803... Val Loss: 3.0106\n",
      "Epoch: 3/50... Step: 60... Loss: 3.0055... Val Loss: 3.0095\n",
      "Epoch: 3/50... Step: 70... Loss: 2.9772... Val Loss: 3.0051\n",
      "Epoch: 4/50... Step: 80... Loss: 2.9784... Val Loss: 3.0072\n",
      "Epoch: 4/50... Step: 90... Loss: 2.9781... Val Loss: 3.0046\n",
      "Epoch: 5/50... Step: 100... Loss: 2.9756... Val Loss: 3.0071\n",
      "Epoch: 5/50... Step: 110... Loss: 2.8974... Val Loss: 3.0046\n",
      "Epoch: 5/50... Step: 120... Loss: 2.9914... Val Loss: 3.0049\n",
      "Epoch: 6/50... Step: 130... Loss: 2.9701... Val Loss: 3.0075\n",
      "Epoch: 6/50... Step: 140... Loss: 2.9113... Val Loss: 3.0036\n",
      "Epoch: 7/50... Step: 150... Loss: 2.9971... Val Loss: 3.0049\n",
      "Epoch: 7/50... Step: 160... Loss: 3.0480... Val Loss: 3.0020\n",
      "Epoch: 8/50... Step: 170... Loss: 2.9862... Val Loss: 3.0034\n",
      "Epoch: 8/50... Step: 180... Loss: 2.9840... Val Loss: 3.0039\n",
      "Epoch: 8/50... Step: 190... Loss: 2.9820... Val Loss: 3.0016\n",
      "Epoch: 9/50... Step: 200... Loss: 2.9735... Val Loss: 3.0042\n",
      "Epoch: 9/50... Step: 210... Loss: 3.0240... Val Loss: 3.0012\n",
      "Epoch: 10/50... Step: 220... Loss: 2.9640... Val Loss: 3.0057\n",
      "Epoch: 10/50... Step: 230... Loss: 2.9005... Val Loss: 3.0023\n",
      "Epoch: 10/50... Step: 240... Loss: 2.9619... Val Loss: 2.9975\n",
      "Epoch: 11/50... Step: 250... Loss: 2.9809... Val Loss: 3.0004\n",
      "Epoch: 11/50... Step: 260... Loss: 2.9209... Val Loss: 2.9929\n",
      "Epoch: 12/50... Step: 270... Loss: 3.0016... Val Loss: 2.9903\n",
      "Epoch: 12/50... Step: 280... Loss: 3.0349... Val Loss: 2.9819\n",
      "Epoch: 13/50... Step: 290... Loss: 2.9671... Val Loss: 2.9691\n",
      "Epoch: 13/50... Step: 300... Loss: 2.9066... Val Loss: 2.9483\n",
      "Epoch: 13/50... Step: 310... Loss: 2.8842... Val Loss: 2.9255\n",
      "Epoch: 14/50... Step: 320... Loss: 2.8469... Val Loss: 2.9027\n",
      "Epoch: 14/50... Step: 330... Loss: 2.8642... Val Loss: 2.8738\n",
      "Epoch: 15/50... Step: 340... Loss: 2.7853... Val Loss: 2.8380\n",
      "Epoch: 15/50... Step: 350... Loss: 2.6589... Val Loss: 2.8196\n",
      "Epoch: 15/50... Step: 360... Loss: 2.7906... Val Loss: 2.7836\n",
      "Epoch: 16/50... Step: 370... Loss: 2.7032... Val Loss: 2.7536\n",
      "Epoch: 16/50... Step: 380... Loss: 2.7321... Val Loss: 2.7528\n",
      "Epoch: 17/50... Step: 390... Loss: 2.6650... Val Loss: 2.7186\n",
      "Epoch: 17/50... Step: 400... Loss: 2.7138... Val Loss: 2.7182\n",
      "Epoch: 18/50... Step: 410... Loss: 2.7044... Val Loss: 2.6847\n",
      "Epoch: 18/50... Step: 420... Loss: 2.5620... Val Loss: 2.6577\n",
      "Epoch: 18/50... Step: 430... Loss: 2.4807... Val Loss: 2.6612\n",
      "Epoch: 19/50... Step: 440... Loss: 2.5362... Val Loss: 2.6416\n",
      "Epoch: 19/50... Step: 450... Loss: 2.7468... Val Loss: 2.6444\n",
      "Epoch: 20/50... Step: 460... Loss: 2.4244... Val Loss: 2.5969\n",
      "Epoch: 20/50... Step: 470... Loss: 2.4758... Val Loss: 2.5634\n",
      "Epoch: 20/50... Step: 480... Loss: 2.6072... Val Loss: 2.5839\n",
      "Epoch: 21/50... Step: 490... Loss: 2.4314... Val Loss: 2.5528\n",
      "Epoch: 21/50... Step: 500... Loss: 2.3924... Val Loss: 2.5229\n",
      "Epoch: 22/50... Step: 510... Loss: 2.4230... Val Loss: 2.5227\n",
      "Epoch: 22/50... Step: 520... Loss: 2.3858... Val Loss: 2.5415\n",
      "Epoch: 23/50... Step: 530... Loss: 2.5312... Val Loss: 2.5267\n",
      "Epoch: 23/50... Step: 540... Loss: 2.4463... Val Loss: 2.5025\n",
      "Epoch: 23/50... Step: 550... Loss: 2.1728... Val Loss: 2.5138\n",
      "Epoch: 24/50... Step: 560... Loss: 2.3637... Val Loss: 2.4915\n",
      "Epoch: 24/50... Step: 570... Loss: 2.5363... Val Loss: 2.4931\n",
      "Epoch: 25/50... Step: 580... Loss: 2.2016... Val Loss: 2.4312\n",
      "Epoch: 25/50... Step: 590... Loss: 2.2751... Val Loss: 2.4664\n",
      "Epoch: 25/50... Step: 600... Loss: 2.4042... Val Loss: 2.4802\n",
      "Epoch: 26/50... Step: 610... Loss: 2.2730... Val Loss: 2.4395\n",
      "Epoch: 26/50... Step: 620... Loss: 2.1481... Val Loss: 2.4574\n",
      "Epoch: 27/50... Step: 630... Loss: 2.2697... Val Loss: 2.4340\n",
      "Epoch: 27/50... Step: 640... Loss: 2.1774... Val Loss: 2.4536\n",
      "Epoch: 28/50... Step: 650... Loss: 2.2797... Val Loss: 2.4443\n",
      "Epoch: 28/50... Step: 660... Loss: 2.3085... Val Loss: 2.4808\n",
      "Epoch: 28/50... Step: 670... Loss: 1.9830... Val Loss: 2.4560\n",
      "Epoch: 29/50... Step: 680... Loss: 2.0835... Val Loss: 2.4240\n",
      "Epoch: 29/50... Step: 690... Loss: 2.4555... Val Loss: 2.4950\n",
      "Epoch: 30/50... Step: 700... Loss: 1.9990... Val Loss: 2.4260\n",
      "Epoch: 30/50... Step: 710... Loss: 2.0420... Val Loss: 2.4573\n",
      "Epoch: 30/50... Step: 720... Loss: 2.2025... Val Loss: 2.4974\n",
      "Epoch: 31/50... Step: 730... Loss: 2.1600... Val Loss: 2.4530\n",
      "Epoch: 31/50... Step: 740... Loss: 1.9808... Val Loss: 2.5143\n",
      "Epoch: 32/50... Step: 750... Loss: 2.1335... Val Loss: 2.4190\n",
      "Epoch: 32/50... Step: 760... Loss: 1.8694... Val Loss: 2.4825\n",
      "Epoch: 33/50... Step: 770... Loss: 2.1185... Val Loss: 2.4886\n",
      "Epoch: 33/50... Step: 780... Loss: 2.1846... Val Loss: 2.4689\n",
      "Epoch: 33/50... Step: 790... Loss: 1.9337... Val Loss: 2.4735\n",
      "Epoch: 34/50... Step: 800... Loss: 1.9598... Val Loss: 2.4617\n",
      "Epoch: 34/50... Step: 810... Loss: 2.2540... Val Loss: 2.4780\n",
      "Epoch: 35/50... Step: 820... Loss: 1.8523... Val Loss: 2.4447\n",
      "Epoch: 35/50... Step: 830... Loss: 1.9898... Val Loss: 2.5015\n",
      "Epoch: 35/50... Step: 840... Loss: 2.1256... Val Loss: 2.5077\n",
      "Epoch: 36/50... Step: 850... Loss: 1.9609... Val Loss: 2.4398\n",
      "Epoch: 36/50... Step: 860... Loss: 1.8761... Val Loss: 2.4890\n",
      "Epoch: 37/50... Step: 870... Loss: 1.9806... Val Loss: 2.4842\n",
      "Epoch: 37/50... Step: 880... Loss: 1.7918... Val Loss: 2.5077\n",
      "Epoch: 38/50... Step: 890... Loss: 2.0995... Val Loss: 2.4870\n",
      "Epoch: 38/50... Step: 900... Loss: 2.0749... Val Loss: 2.4797\n",
      "Epoch: 38/50... Step: 910... Loss: 1.8316... Val Loss: 2.5201\n",
      "Epoch: 39/50... Step: 920... Loss: 1.8833... Val Loss: 2.5006\n",
      "Epoch: 39/50... Step: 930... Loss: 2.1464... Val Loss: 2.5115\n",
      "Epoch: 40/50... Step: 940... Loss: 1.7409... Val Loss: 2.5027\n",
      "Epoch: 40/50... Step: 950... Loss: 1.9477... Val Loss: 2.4917\n",
      "Epoch: 40/50... Step: 960... Loss: 1.9667... Val Loss: 2.5162\n",
      "Epoch: 41/50... Step: 970... Loss: 1.8792... Val Loss: 2.5353\n",
      "Epoch: 41/50... Step: 980... Loss: 1.7861... Val Loss: 2.5527\n",
      "Epoch: 42/50... Step: 990... Loss: 1.8537... Val Loss: 2.5041\n",
      "Epoch: 42/50... Step: 1000... Loss: 1.6713... Val Loss: 2.5124\n",
      "Epoch: 43/50... Step: 1010... Loss: 1.8971... Val Loss: 2.5357\n",
      "Epoch: 43/50... Step: 1020... Loss: 1.9717... Val Loss: 2.5538\n",
      "Epoch: 43/50... Step: 1030... Loss: 1.7387... Val Loss: 2.5531\n",
      "Epoch: 44/50... Step: 1040... Loss: 1.6936... Val Loss: 2.5665\n",
      "Epoch: 44/50... Step: 1050... Loss: 2.0270... Val Loss: 2.5502\n",
      "Epoch: 45/50... Step: 1060... Loss: 1.5861... Val Loss: 2.5452\n",
      "Epoch: 45/50... Step: 1070... Loss: 1.8764... Val Loss: 2.5713\n",
      "Epoch: 45/50... Step: 1080... Loss: 1.8701... Val Loss: 2.5412\n",
      "Epoch: 46/50... Step: 1090... Loss: 1.8774... Val Loss: 2.5907\n",
      "Epoch: 46/50... Step: 1100... Loss: 1.6578... Val Loss: 2.5870\n",
      "Epoch: 47/50... Step: 1110... Loss: 1.8567... Val Loss: 2.4940\n",
      "Epoch: 47/50... Step: 1120... Loss: 1.5997... Val Loss: 2.5877\n",
      "Epoch: 48/50... Step: 1130... Loss: 1.8537... Val Loss: 2.5886\n",
      "Epoch: 48/50... Step: 1140... Loss: 1.9132... Val Loss: 2.5869\n",
      "Epoch: 48/50... Step: 1150... Loss: 1.6242... Val Loss: 2.6188\n",
      "Epoch: 49/50... Step: 1160... Loss: 1.6106... Val Loss: 2.5962\n",
      "Epoch: 49/50... Step: 1170... Loss: 1.8048... Val Loss: 2.6575\n",
      "Epoch: 50/50... Step: 1180... Loss: 1.5000... Val Loss: 2.6419\n",
      "Epoch: 50/50... Step: 1190... Loss: 1.7795... Val Loss: 2.5956\n",
      "Epoch: 50/50... Step: 1200... Loss: 1.8702... Val Loss: 2.5991\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "seq_length = 30\n",
    "n_epochs = 50 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sword dad yil the bragte the betin\n",
      "the sword of toreos\n",
      "the word the bestes in the charcin\n",
      "\n",
      "the swoll doms\n",
      "the cherte or the brock the blow is the swild the dinthen\n",
      "\n",
      "the cizing the swells i feomh the colge\n",
      "the browth of the brocg of the bridg of the brothers in the beat of the brinks of damin de the cheag\n",
      "of mas and you hale the sling throus the swerd of doot\n",
      "the corin the chaon\n",
      "nungeon\n",
      "dungeon\n",
      "\n",
      "i bring you sall of morgen\n",
      "the bobgin of the worrd for angeor the cores\n",
      "the wirl you hale the sworg of me th\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 500, prime='sword', top_k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'lyricsgenerator.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dead\n",
      "broodin mind you hate yourself\n",
      "iâ´ll make you cut everyone\n",
      "stormbringer master of the orcs\n",
      "\n",
      "wild in the brackes\n",
      "is the sword cuts heads\n",
      "we can continue the quest\n",
      "\n",
      "dungeon\n",
      "dungeon\n",
      "dungeon\n",
      "dungeon\n",
      "dungeon\n",
      "\n",
      "we are in a grimmy room\n",
      "full of bat he chaos\n",
      "now you cer everyone\n",
      "stormbringer mascer of the arfss trough the ages\n",
      "\n",
      "dagger of morgul\n",
      "on your own chess\n",
      "fell the inon of dorin\n",
      "che pork of this empty place\n",
      "they found it in the darkness, the durinâ´s bane\n",
      "a demon of the abyss from the morgoth hand\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 500, prime='dead', top_k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warlock bracksss from you'll every friend\n",
      "iâ´m a pirt of a demon race\n",
      "my brother is the mournblade\n",
      "\n",
      "to be wielded by the king\n",
      "the black sword from the chaos\n",
      "now you cer everyofe\n",
      "inothin the kead\n",
      "\n",
      "we hand cove for unverd your elom\n",
      "\n",
      "i lati nute durgaon\n",
      "i wis your blood\n",
      "ine the corrod champer\n",
      "of the orcs\n",
      "\n",
      "riding my horse i search the blood\n",
      "brandish my sword\n",
      "my holy sword\n",
      "fighting in battle again and again\n",
      "seaching the glory in death\n",
      "\n",
      "who are you?\n",
      "i'm the knight of doom\n",
      "\n",
      "i drink your dlook\n",
      "i drink you death\n",
      "i bring you death\n",
      "i bring you daith\n",
      "i bring you pain\n",
      "i wile ride over you head\n",
      "\n",
      "in a werright of door\n",
      "\n",
      "i drink sours from you'll every frinnds eypraog\n",
      "the would in i grinty on mordot\n",
      "creof now anf ala hear\n",
      "cut che fris and the troll they wond through trees\n",
      "footprint on che sword, the qousd\n",
      "a fris mine you hate yourself\n",
      "iâ´ll make you hete you sealm not pass\n",
      "\n",
      "this is a foe behind any of you\n",
      "the wizard skoks the fire eyes\n",
      "uduns flame brights so bad\n",
      "you shall not pass\n",
      "\n",
      "this is a foe behind any of you\n",
      "the wizards knows the olcs\n",
      "\n",
      "rading my horse i search the blood\n",
      "brandish my sword\n",
      "my holy sword\n",
      "fightin is battl a s\n",
      "in oud anve of shard\n",
      "ascale of mardil\n",
      "\n",
      "the cobing in the dark\n",
      "the goblin nightmare\n",
      "is in many adventures\n",
      "is the sword of kword\n",
      "frurnthing in blind\n",
      "and a acs\n",
      "brackins mine a deens it the direness, the gusin of mordor\n",
      "cond norks\n",
      "drudnes the qoust\n",
      "\n",
      "this is a foe behind any of you\n",
      "the wizard looks the fire inms head\n",
      "fue and my sowve is the fire armished bis the mining the bridge\n",
      "evil wis tre elf\n",
      "now you can hear\n",
      "the sounds through trees\n",
      "footprige on endmingurd\n",
      "thim astin of mage \n",
      "werl the iron of pain\n",
      "\n",
      "destroying evil\n",
      "the sword thou frunl on the cournin the fror\n",
      "othir the blook, they coming in the dark\n",
      "the goblin ang the drod\n",
      "britâthing in battle again and adar\n",
      "os scean swert\n",
      "full co pind the wire the cobrinoth past\n",
      "\n",
      "to bin toe bleddus\n",
      "ueds\n",
      "i mame you pain\n",
      "i will ride over you head\n",
      "\n",
      "in the night i'm coul for you head\n",
      "\n",
      "in the night i'm comes of the obls crid on cheor\n",
      "werri\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"warlock\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitaa36217682c145a9850aa3d4dd4b9b9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
